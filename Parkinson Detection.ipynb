{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from math import sqrt\n",
    "from math import ceil\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_data_path=os.path.join('data', 'control')\n",
    "parkinson_data_path=os.path.join('data', 'parkinson') #Path of parkinson data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_file_list=[os.path.join(control_data_path, x) for x in os.listdir(control_data_path)]\n",
    "parkinson_file_list=[os.path.join(parkinson_data_path, x) for x in os.listdir(parkinson_data_path)] #Path of parkinson data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_row=[\"X\", \"Y\", \"Z\", \"Pressure\" , \"GripAngle\" , \"Timestamp\" , \"Test_ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_no_strokes(df):\n",
    "    pressure_data=df['Pressure'].values\n",
    "    on_surface = (pressure_data>600).astype(int)\n",
    "    return ((np.roll(on_surface, 1) - on_surface) != 0).astype(int).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speed(df):\n",
    "    total_dist=0\n",
    "    duration=df['Timestamp'].values[-1]\n",
    "    coords=df[['X', 'Y', 'Z']].values\n",
    "    for i in range(10, df.shape[0]):\n",
    "        temp=np.linalg.norm(coords[i, :]-coords[i-10, :])\n",
    "        total_dist+=temp\n",
    "    speed=total_dist/duration\n",
    "    return speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_in_air_time(data):\n",
    "    data=data['Pressure'].values\n",
    "    return (data<600).astype(int).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_on_surface_time(data):\n",
    "    data=data['Pressure'].values\n",
    "    return (data>600).astype(int).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_velocity(f):\n",
    "    data_pat=f\n",
    "    Vel = []\n",
    "    horz_Vel = []\n",
    "    horz_vel_mag = []\n",
    "    vert_vel_mag = []\n",
    "    vert_Vel = []\n",
    "    magnitude = []\n",
    "    timestamp_diff =  []\n",
    "    \n",
    "    t = 0\n",
    "    for i in range(len(data_pat)-2):\n",
    "        if t+10 <= len(data_pat)-1:\n",
    "            Vel.append(((data_pat['X'].values[t+10] - data_pat['X'].values[t])/(data_pat['Timestamp'].values[t+10]-data_pat['Timestamp'].values[t]) , (data_pat['Y'].values[t+10]-data_pat['Y'].values[t])/(data_pat['Timestamp'].values[t+10]-data_pat['Timestamp'].values[t])))\n",
    "            horz_Vel.append((data_pat['X'].values[t+10] - data_pat['X'].values[t])/(data_pat['Timestamp'].values[t+10]-data_pat['Timestamp'].values[t]))\n",
    "            \n",
    "            vert_Vel.append((data_pat['Y'].values[t+10] - data_pat['Y'].values[t])/(data_pat['Timestamp'].values[t+10]-data_pat['Timestamp'].values[t]))\n",
    "            magnitude.append(sqrt(((data_pat['X'].values[t+10]-data_pat['X'].values[t])/(data_pat['Timestamp'].values[t+10]-data_pat['Timestamp'].values[t]))**2 + (((data_pat['Y'].values[t+10]-data_pat['Y'].values[t])/(data_pat['Timestamp'].values[t+10]-data_pat['Timestamp'].values[t]))**2)))\n",
    "            timestamp_diff.append(data_pat['Timestamp'].values[t+10]-data_pat['Timestamp'].values[t])\n",
    "            horz_vel_mag.append(abs(horz_Vel[len(horz_Vel)-1]))\n",
    "            vert_vel_mag.append(abs(vert_Vel[len(vert_Vel)-1]))\n",
    "            t = t+10\n",
    "        else:\n",
    "            break\n",
    "    magnitude_vel = np.mean(magnitude)  \n",
    "    magnitude_horz_vel = np.mean(horz_vel_mag)\n",
    "    magnitude_vert_vel = np.mean(vert_vel_mag)\n",
    "    return Vel,magnitude,timestamp_diff,horz_Vel,vert_Vel,magnitude_vel,magnitude_horz_vel,magnitude_vert_vel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_acceleration(f):\n",
    "    Vel,magnitude,timestamp_diff,horz_Vel,vert_Vel,magnitude_vel,magnitude_horz_vel,magnitude_vert_vel = find_velocity(f)\n",
    "    accl = []\n",
    "    horz_Accl =  []\n",
    "    vert_Accl = []\n",
    "    magnitude = []\n",
    "    horz_acc_mag = []\n",
    "    vert_acc_mag = []\n",
    "    for i in range(len(Vel)-2):\n",
    "        accl.append(((Vel[i+1][0]-Vel[i][0])/timestamp_diff[i] , (Vel[i+1][1]-Vel[i][1])/timestamp_diff[i]))\n",
    "        horz_Accl.append((horz_Vel[i+1]-horz_Vel[i])/timestamp_diff[i])\n",
    "        vert_Accl.append((vert_Vel[i+1]-vert_Vel[i])/timestamp_diff[i])\n",
    "        horz_acc_mag.append(abs(horz_Accl[len(horz_Accl)-1]))\n",
    "        vert_acc_mag.append(abs(vert_Accl[len(vert_Accl)-1]))\n",
    "        magnitude.append(sqrt(((Vel[i+1][0]-Vel[i][0])/timestamp_diff[i])**2 + ((Vel[i+1][1]-Vel[i][1])/timestamp_diff[i])**2))\n",
    "    \n",
    "    magnitude_acc = np.mean(magnitude)  \n",
    "    magnitude_horz_acc = np.mean(horz_acc_mag)\n",
    "    magnitude_vert_acc = np.mean(vert_acc_mag)\n",
    "    return accl,magnitude,horz_Accl,vert_Accl,timestamp_diff,magnitude_acc,magnitude_horz_acc,magnitude_vert_acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_jerk(f):\n",
    "    accl,magnitude,horz_Accl,vert_Accl,timestamp_diff,magnitude_acc,magnitude_horz_acc,magnitude_vert_acc = find_acceleration(f)\n",
    "    jerk = []\n",
    "    hrz_jerk = []\n",
    "    vert_jerk = []\n",
    "    magnitude = []\n",
    "    horz_jerk_mag = []\n",
    "    vert_jerk_mag = []\n",
    "    \n",
    "    for i in range(len(accl)-2):\n",
    "        jerk.append(((accl[i+1][0]-accl[i][0])/timestamp_diff[i] , (accl[i+1][1]-accl[i][1])/timestamp_diff[i]))\n",
    "        hrz_jerk.append((horz_Accl[i+1]-horz_Accl[i])/timestamp_diff[i])\n",
    "        vert_jerk.append((vert_Accl[i+1]-vert_Accl[i])/timestamp_diff[i])\n",
    "        horz_jerk_mag.append(abs(hrz_jerk[len(hrz_jerk)-1]))\n",
    "        vert_jerk_mag.append(abs(vert_jerk[len(vert_jerk)-1]))\n",
    "        magnitude.append(sqrt(((accl[i+1][0]-accl[i][0])/timestamp_diff[i])**2 + ((accl[i+1][1]-accl[i][1])/timestamp_diff[i])**2))\n",
    "        \n",
    "    magnitude_jerk = np.mean(magnitude)  \n",
    "    magnitude_horz_jerk = np.mean(horz_jerk_mag)\n",
    "    magnitude_vert_jerk = np.mean(vert_jerk_mag)\n",
    "    return jerk,magnitude,hrz_jerk,vert_jerk,timestamp_diff,magnitude_jerk,magnitude_horz_jerk,magnitude_vert_jerk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NCV_per_halfcircle(f):\n",
    "    data_pat=f\n",
    "    Vel = []\n",
    "    ncv = []\n",
    "    temp_ncv = 0\n",
    "    basex = data_pat['X'].values[0]\n",
    "    for i in range(len(data_pat)-2):\n",
    "        if data_pat['X'].values[i] == basex:\n",
    "            ncv.append(temp_ncv)\n",
    "            temp_ncv = 0\n",
    "            continue\n",
    "            \n",
    "        Vel.append(((data_pat['X'].values[i+1] - data_pat['X'].values[i])/(data_pat['Timestamp'].values[i+1]-data_pat['Timestamp'].values[i]) , (data_pat['Y'].values[i+1]-data_pat['Y'].values[i])/(data_pat['Timestamp'].values[i+1]-data_pat['Timestamp'].values[i])))\n",
    "        if Vel[len(Vel)-1] != (0,0):\n",
    "            temp_ncv+=1\n",
    "    ncv.append(temp_ncv)\n",
    "    #ncv = list(filter((2).__ne__, ncv))\n",
    "    ncv_Val = np.sum(ncv)/np.count_nonzero(ncv)\n",
    "    return ncv,ncv_Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NCA_per_halfcircle(f):\n",
    "    data_pat=f\n",
    "    Vel,magnitude,timestamp_diff,horz_Vel,vert_Vel,magnitude_vel,magnitude_horz_vel,magnitude_vert_vel = find_velocity(f)\n",
    "    accl = []\n",
    "    nca = []\n",
    "    temp_nca = 0\n",
    "    basex = data_pat['X'].values[0]\n",
    "    for i in range(len(Vel)-2):\n",
    "        if data_pat['X'].values[i] == basex:\n",
    "            nca.append(temp_nca)\n",
    "            #print ('tempNCa::',temp_nca)\n",
    "            temp_nca = 0\n",
    "            continue\n",
    "            \n",
    "        accl.append(((Vel[i+1][0]-Vel[i][0])/timestamp_diff[i] , (Vel[i+1][1]-Vel[i][1])/timestamp_diff[i]))\n",
    "        if accl[len(accl)-1] != (0,0):\n",
    "            temp_nca+=1\n",
    "    nca.append(temp_nca)\n",
    "    nca = list(filter((2).__ne__, nca))\n",
    "    nca_Val = np.sum(nca)/np.count_nonzero(nca)\n",
    "    return nca,nca_Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(f, parkinson_target):\n",
    "    global header_row\n",
    "    df=pd.read_csv(f, sep=';', header=None, names=header_row)\n",
    "    \n",
    "    df_static=df[df[\"Test_ID\"]==0]    # static test\n",
    "    df_dynamic=df[df[\"Test_ID\"]==1]    # dynamic test\n",
    "    df_stcp=df[df[\"Test_ID\"]==2]    # STCP\n",
    "    #df_static_dynamic=pd.concat([df_static, df_dynamic])\n",
    "    \n",
    "    initial_timestamp=df['Timestamp'][0]\n",
    "    df['Timestamp']=df['Timestamp']- initial_timestamp # offset timestamps\n",
    "    \n",
    "    duration_static = df_static['Timestamp'].values[-1] if df_static.shape[0] else 1\n",
    "    duration_dynamic = df_dynamic['Timestamp'].values[-1] if df_dynamic.shape[0] else 1\n",
    "    duration_STCP = df_stcp['Timestamp'].values[-1] if df_stcp.shape[0] else 1\n",
    "\n",
    "    \n",
    "    data_point=[]\n",
    "    data_point.append(get_no_strokes(df_static) if df_static.shape[0] else 0) # no. of strokes for static test\n",
    "    data_point.append(get_no_strokes(df_dynamic) if df_dynamic.shape[0] else 0) # no. of strokes for dynamic test\n",
    "    data_point.append(get_speed(df_static) if df_static.shape[0] else 0) # speed for static test\n",
    "    data_point.append(get_speed(df_dynamic) if df_dynamic.shape[0] else 0) # speed for dynamic test\n",
    "\n",
    "    Vel,magnitude,timestamp_diff,horz_Vel,vert_Vel,magnitude_vel,magnitude_horz_vel,magnitude_vert_vel = find_velocity(df_static) if df_static.shape[0] else (0,0,0,0,0,0,0,0) # magnitudes of velocity, horizontal velocity and vertical velocity for static test\n",
    "    data_point.extend([magnitude_vel, magnitude_horz_vel,magnitude_vert_vel])\n",
    "    Vel,magnitude,timestamp_diff,horz_Vel,vert_Vel,magnitude_vel,magnitude_horz_vel,magnitude_vert_vel = find_velocity(df_dynamic) if df_dynamic.shape[0] else (0,0,0,0,0,0,0,0) # magnitudes of velocity, horizontal velocity and vertical velocity for dynamic test\n",
    "    data_point.extend([magnitude_vel, magnitude_horz_vel,magnitude_vert_vel])\n",
    "    \n",
    "    accl,magnitude,horz_Accl,vert_Accl,timestamp_diff,magnitude_acc,magnitude_horz_acc,magnitude_vert_acc=find_acceleration(df_static) if df_static.shape[0] else (0,0,0,0,0,0,0,0)# magnitudes of acceleration, horizontal acceleration and vertical acceleration for static test        \n",
    "    data_point.extend([magnitude_acc,magnitude_horz_acc,magnitude_vert_acc])\n",
    "    accl,magnitude,horz_Accl,vert_Accl,timestamp_diff,magnitude_acc,magnitude_horz_acc,magnitude_vert_acc=find_acceleration(df_dynamic) if df_dynamic.shape[0] else (0,0,0,0,0,0,0,0)# magnitudes of acceleration, horizontal acceleration and vertical acceleration for dynamic test        \n",
    "    data_point.extend([magnitude_acc,magnitude_horz_acc,magnitude_vert_acc])\n",
    "    \n",
    "    jerk,magnitude,hrz_jerk,vert_jerk,timestamp_diff,magnitude_jerk,magnitude_horz_jerk,magnitude_vert_jerk=find_jerk(df_static) if df_static.shape[0] else (0,0,0,0,0,0,0,0) # magnitudes of jerk, horizontal jerk and vertical jerk for static test\n",
    "    data_point.extend([magnitude_jerk,magnitude_horz_jerk,magnitude_vert_jerk])\n",
    "    jerk,magnitude,hrz_jerk,vert_jerk,timestamp_diff,magnitude_jerk,magnitude_horz_jerk,magnitude_vert_jerk=find_jerk(df_dynamic) if df_dynamic.shape[0] else (0,0,0,0,0,0,0,0) # magnitudes of jerk, horizontal jerk and vertical jerk for dynamic test\n",
    "    data_point.extend([magnitude_jerk,magnitude_horz_jerk,magnitude_vert_jerk])\n",
    "    \n",
    "    ncv,ncv_Val=NCV_per_halfcircle(df_static) if df_static.shape[0] else (0,0) # NCV for static test\n",
    "    data_point.append(ncv_Val)\n",
    "    ncv,ncv_Val=NCV_per_halfcircle(df_dynamic) if df_dynamic.shape[0] else (0,0) # NCV for dynamic test\n",
    "    data_point.append(ncv_Val)\n",
    "    \n",
    "    nca,nca_Val=NCA_per_halfcircle(df_static) if df_static.shape[0] else (0,0) # NCA for static test\n",
    "    data_point.append(nca_Val)\n",
    "    nca,nca_Val=NCA_per_halfcircle(df_dynamic) if df_dynamic.shape[0] else (0,0) # NCA for dynamic test\n",
    "    data_point.append(nca_Val)\n",
    "    \n",
    "    data_point.append(get_in_air_time(df_stcp) if df_stcp.shape[0] else 0) # in air time for STCP\n",
    "    data_point.append(get_on_surface_time(df_static) if df_static.shape[0] else 0) # on surface time for static test\n",
    "    data_point.append(get_on_surface_time(df_dynamic) if df_dynamic.shape[0] else 0) # on surface time for dynamic test\n",
    "    \n",
    "    data_point.append(parkinson_target)    # traget. 1 for parkinson. 0 for control.\n",
    "    \n",
    "    return data_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_v2(f, parkinson_target):\n",
    "    global header_row\n",
    "    df=pd.read_csv(f, sep=';', header=None, names=header_row)\n",
    "    \n",
    "    df_static=df[df[\"Test_ID\"]==0]    # static test\n",
    "    df_dynamic=df[df[\"Test_ID\"]==1]    # dynamic test\n",
    "    df_stcp=df[df[\"Test_ID\"]==2]    # STCP\n",
    "    #df_static_dynamic=pd.concat([df_static, df_dynamic])\n",
    "    \n",
    "    initial_timestamp=df['Timestamp'][0]\n",
    "    df['Timestamp']=df['Timestamp']- initial_timestamp # offset timestamps\n",
    "    \n",
    "    duration_static = df_static['Timestamp'].values[-1] if df_static.shape[0] else 1\n",
    "    duration_dynamic = df_dynamic['Timestamp'].values[-1] if df_dynamic.shape[0] else 1\n",
    "    duration_STCP = df_stcp['Timestamp'].values[-1] if df_stcp.shape[0] else 1\n",
    "\n",
    "    segment_size=20\n",
    "    data_point=[]\n",
    "    \n",
    "    temp_array=[]\n",
    " \n",
    "    data_point.append(get_no_strokes(df_static) if df_static.shape[0] else 0) # no. of strokes for static test\n",
    "    data_point.append(get_no_strokes(df_dynamic) if df_dynamic.shape[0] else 0) # no. of strokes for dynamic test\n",
    "    data_point.append(get_speed(df_static) if df_static.shape[0] else 0) # speed for static test\n",
    "    data_point.append(get_speed(df_dynamic) if df_dynamic.shape[0] else 0) # speed for dynamic test\n",
    " \n",
    "    Vel,magnitude,timestamp_diff,horz_Vel,vert_Vel,magnitude_vel,magnitude_horz_vel,magnitude_vert_vel = find_velocity(df_static) if df_static.shape[0] else (0,0,0,0,0,0,0,0) # magnitudes of velocity, horizontal velocity and vertical velocity for static test\n",
    "    data_point.extend([magnitude_vel, magnitude_horz_vel,magnitude_vert_vel])\n",
    "    Vel,magnitude,timestamp_diff,horz_Vel,vert_Vel,magnitude_vel,magnitude_horz_vel,magnitude_vert_vel = find_velocity(df_dynamic) if df_dynamic.shape[0] else (0,0,0,0,0,0,0,0) # magnitudes of velocity, horizontal velocity and vertical velocity for dynamic test\n",
    "    data_point.extend([magnitude_vel, magnitude_horz_vel,magnitude_vert_vel])\n",
    "    \n",
    "    accl,magnitude,horz_Accl,vert_Accl,timestamp_diff,magnitude_acc,magnitude_horz_acc,magnitude_vert_acc=find_acceleration(df_static) if df_static.shape[0] else (0,0,0,0,0,0,0,0)# magnitudes of acceleration, horizontal acceleration and vertical acceleration for static test        \n",
    "    data_point.extend([magnitude_acc,magnitude_horz_acc,magnitude_vert_acc])\n",
    "    accl,magnitude,horz_Accl,vert_Accl,timestamp_diff,magnitude_acc,magnitude_horz_acc,magnitude_vert_acc=find_acceleration(df_dynamic) if df_dynamic.shape[0] else (0,0,0,0,0,0,0,0)# magnitudes of acceleration, horizontal acceleration and vertical acceleration for dynamic test        \n",
    "    data_point.extend([magnitude_acc,magnitude_horz_acc,magnitude_vert_acc])\n",
    "    \n",
    "    jerk,magnitude,hrz_jerk,vert_jerk,timestamp_diff,magnitude_jerk,magnitude_horz_jerk,magnitude_vert_jerk=find_jerk(df_static) if df_static.shape[0] else (0,0,0,0,0,0,0,0) # magnitudes of jerk, horizontal jerk and vertical jerk for static test\n",
    "    data_point.extend([magnitude_jerk,magnitude_horz_jerk,magnitude_vert_jerk])\n",
    "    jerk,magnitude,hrz_jerk,vert_jerk,timestamp_diff,magnitude_jerk,magnitude_horz_jerk,magnitude_vert_jerk=find_jerk(df_dynamic) if df_dynamic.shape[0] else (0,0,0,0,0,0,0,0) # magnitudes of jerk, horizontal jerk and vertical jerk for dynamic test\n",
    "    data_point.extend([magnitude_jerk,magnitude_horz_jerk,magnitude_vert_jerk])\n",
    "    \n",
    "    ncv,ncv_Val=NCV_per_halfcircle(df_static) if df_static.shape[0] else (0,0) # NCV for static test\n",
    "    data_point.append(ncv_Val)\n",
    "    ncv,ncv_Val=NCV_per_halfcircle(df_dynamic) if df_dynamic.shape[0] else (0,0) # NCV for dynamic test\n",
    "    data_point.append(ncv_Val)\n",
    "    \n",
    "    nca,nca_Val=NCA_per_halfcircle(df_static) if df_static.shape[0] else (0,0) # NCA for static test\n",
    "    data_point.append(nca_Val)\n",
    "    nca,nca_Val=NCA_per_halfcircle(df_dynamic) if df_dynamic.shape[0] else (0,0) # NCA for dynamic test\n",
    "    data_point.append(nca_Val)\n",
    "     \n",
    "    temp_array.clear()\n",
    "    count = ceil(df_stcp.shape[0]/segment_size)\n",
    "    for i in range (count):\n",
    "        temp_array.append(get_in_air_time(df_stcp[i*segment_size: (i+1)*segment_size - 1]) if df_stcp.shape[0] else 0)\n",
    "    data_point.append(statistics.mean(temp_array) if len(temp_array) else 0)\n",
    "    data_point.append(statistics.stdev(temp_array) if len(temp_array) else 0)\n",
    "    \n",
    "    \n",
    "    temp_array.clear()\n",
    "    count = ceil(df_static.shape[0]/segment_size)\n",
    "    for i in range (count):\n",
    "        temp_array.append(get_on_surface_time(df_static[i*segment_size: (i+1)*segment_size - 1]) if df_static.shape[0] else 0) \n",
    "    data_point.append(statistics.mean(temp_array) if len(temp_array) else 0)\n",
    "    data_point.append(statistics.stdev(temp_array) if len(temp_array) else 0)\n",
    "    \n",
    "    temp_array.clear()\n",
    "    count = ceil(df_dynamic.shape[0]/segment_size)\n",
    "    for i in range (count):\n",
    "        temp_array.append(get_on_surface_time(df_dynamic[i*segment_size: (i+1)*segment_size - 1])if df_dynamic.shape[0] else 0) # no. of strokes for static test\n",
    "    data_point.append(statistics.mean(temp_array) if len(temp_array) else 0)\n",
    "    data_point.append(statistics.stdev(temp_array) if len(temp_array) else 0)\n",
    "    \n",
    "    data_point.append(parkinson_target)    # traget. 1 for parkinson. 0 for control.\n",
    "    \n",
    "    return data_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw=[]\n",
    "for x in parkinson_file_list:\n",
    "    raw.append(get_features_v2(x, 1))\n",
    "for x in control_file_list:\n",
    "    raw.append(get_features_v2(x, 0))\n",
    "#print(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw=np.array(raw)\n",
    "print(len(raw))\n",
    "print(len(raw[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_headers=['no_strokes_st', 'no_strokes_dy', 'speed_st', 'speed_dy', 'magnitude_vel_st' , 'magnitude_horz_vel_st' , 'magnitude_vert_vel_st', 'magnitude_vel_dy', 'magnitude_horz_vel_dy' , 'magnitude_vert_vel_dy', 'magnitude_acc_st' , 'magnitude_horz_acc_st' , 'magnitude_vert_acc_st','magnitude_acc_dy' , 'magnitude_horz_acc_dy' , 'magnitude_vert_acc_dy', 'magnitude_jerk_st', 'magnitude_horz_jerk_st' , 'magnitude_vert_jerk_st', 'magnitude_jerk_dy', 'magnitude_horz_jerk_dy' , 'magnitude_vert_jerk_dy', 'ncv_st', 'ncv_dy', 'nca_st', 'nca_dy', 'in_air_stcp','on_surface_st', 'on_surface_dy', 'target']\n",
    "new_features_headers=['no_strokes_st', 'no_strokes_dy', 'speed_st', 'speed_dy', \n",
    "                  'magnitude_vel_st' , 'magnitude_horz_vel_st' , 'magnitude_vert_vel_st', 'magnitude_vel_dy', 'magnitude_horz_vel_dy' , 'magnitude_vert_vel_dy', \n",
    "                  'magnitude_acc_st' , 'magnitude_horz_acc_st' , 'magnitude_vert_acc_st','magnitude_acc_dy' , 'magnitude_horz_acc_dy' , 'magnitude_vert_acc_dy', \n",
    "                  'magnitude_jerk_st', 'magnitude_horz_jerk_st' , 'magnitude_vert_jerk_st', 'magnitude_jerk_dy', 'magnitude_horz_jerk_dy' , 'magnitude_vert_jerk_dy',\n",
    "                  'ncv_st', 'ncv_dy', 'nca_st', 'nca_dy',\n",
    "                  'in_air_stcp_mean', 'in_air_stcp_stdev', 'on_surface_st_mean', 'on_surface_st_stdev', 'on_surface_dy_mean', 'on_surface_dy_stdev',\n",
    "                  'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame(raw, columns=new_features_headers)\n",
    "data.fillna(0, inplace=True)\n",
    "data.to_csv('info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos=data[data['target']==1]\n",
    "neg=data[data['target']==0]\n",
    "\n",
    "train_pos=pos.tail(pos.shape[0]-5)\n",
    "train_neg=neg.tail(pos.shape[0]-5)\n",
    "train=pd.concat([train_pos, train_neg])\n",
    "print('train shape', train.shape)\n",
    "\n",
    "test_pos=pos.head(5)\n",
    "test_neg=neg.head(5)\n",
    "test=pd.concat([test_pos, test_neg])\n",
    "\n",
    "\n",
    "train_y=train['target']\n",
    "train_x=train.drop(['target'], axis=1)\n",
    "test_y=test['target']\n",
    "test_x=test.drop(['target'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(prediction,actual):\n",
    "    correct = 0\n",
    "    not_correct = 0\n",
    "    for i in range(len(prediction)):\n",
    "        if prediction[i] == actual[i]:\n",
    "            correct+=1\n",
    "        else:\n",
    "            not_correct+=1\n",
    "    return (correct*100)/(correct+not_correct)\n",
    "\n",
    "\n",
    "def metrics(prediction,actual):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for i in range(len(prediction)):\n",
    "        if prediction[i] == actual[i] and actual[i]==1:\n",
    "            tp+=1\n",
    "        if prediction[i] == actual[i] and actual[i]==0:\n",
    "            tn+=1\n",
    "        if prediction[i] != actual[i] and actual[i]==0:\n",
    "            fp+=1\n",
    "        if prediction[i] != actual[i] and actual[i]==1:\n",
    "            fn+=1\n",
    "    metrics = {'Precision':(tp/(tp+fp+tn+fn)),'Recall':(tp/(tp+fn)),'F1':(2*(tp/(tp+fp+tn+fn))*(tp/(tp+fn)))/((tp/(tp+fp+tn+fn))+(tp/(tp+fn)))}\n",
    "    return (metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_info=[]\n",
    "result_info_headers=['Algorithom', 'Test Accuracy', 'Test Precision', 'Test Recall', 'Test F1', 'Train Accuracy', 'Train Precision', 'Train Recall', 'Train F1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_result(algorithmName, testAccuracy, testMetrics, trainAccuracy, trainMetrics):\n",
    "    result=[]\n",
    "    result.append(algorithmName)\n",
    "    result.append(testAccuracy)\n",
    "    result.append(testMetrics['Precision'])\n",
    "    result.append(testMetrics['Recall'])\n",
    "    result.append(testMetrics['F1'])\n",
    "    result.append(trainAccuracy)\n",
    "    result.append(trainMetrics['Precision'])\n",
    "    result.append(trainMetrics['Recall'])\n",
    "    result.append(trainMetrics['F1'])\n",
    "    result_info.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "clf.fit(train_x, train_y)\n",
    "\n",
    "preds=clf.predict(test_x)\n",
    "testAccuracy = accuracy(test_y.tolist(), preds.tolist())\n",
    "print('TestAccuracy:',testAccuracy, '%')\n",
    "testMetrics = metrics(test_y.tolist(), preds.tolist())\n",
    "print(testMetrics)\n",
    "\n",
    "pred=clf.predict(train_x)\n",
    "trainAccuracy = accuracy(train_y.tolist(), pred.tolist())\n",
    "print('TrainAccuracy:',trainAccuracy, '%')\n",
    "trainMetrics = metrics(train_y.tolist(), pred.tolist())\n",
    "print(trainMetrics)\n",
    "\n",
    "add_result('LogisticRegression', testAccuracy, testMetrics, trainAccuracy, trainMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=RandomForestClassifier()\n",
    "clf.fit(train_x, train_y)\n",
    "preds=clf.predict(test_x)\n",
    "testAccuracy = accuracy(test_y.tolist(), preds.tolist())\n",
    "print('TestAccuracy:',testAccuracy, '%')\n",
    "testMetrics = metrics(test_y.tolist(), preds.tolist())\n",
    "print(testMetrics)\n",
    "\n",
    "pred=clf.predict(train_x)\n",
    "trainAccuracy = accuracy(train_y.tolist(), pred.tolist())\n",
    "print('TrainAccuracy:',trainAccuracy, '%')\n",
    "trainMetrics = metrics(train_y.tolist(), pred.tolist())\n",
    "print(trainMetrics)\n",
    "\n",
    "add_result('RandomForest', testAccuracy, testMetrics, trainAccuracy, trainMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=KNeighborsClassifier()\n",
    "clf.fit(train_x, train_y)\n",
    "preds=clf.predict(test_x)\n",
    "testAccuracy = accuracy(test_y.tolist(), preds.tolist())\n",
    "print('TestAccuracy:',testAccuracy, '%')\n",
    "testMetrics = metrics(test_y.tolist(), preds.tolist())\n",
    "print(testMetrics)\n",
    "\n",
    "pred=clf.predict(train_x)\n",
    "trainAccuracy = accuracy(train_y.tolist(), pred.tolist())\n",
    "print('TrainAccuracy:',trainAccuracy, '%')\n",
    "trainMetrics = metrics(train_y.tolist(), pred.tolist())\n",
    "print(trainMetrics)\n",
    "\n",
    "add_result('KNeighbors', testAccuracy, testMetrics, trainAccuracy, trainMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=DecisionTreeClassifier()\n",
    "clf.fit(train_x, train_y)\n",
    "preds=clf.predict(test_x)\n",
    "testAccuracy = accuracy(test_y.tolist(), preds.tolist())\n",
    "print('TestAccuracy:',testAccuracy, '%')\n",
    "testMetrics = metrics(test_y.tolist(), preds.tolist())\n",
    "print(testMetrics)\n",
    "\n",
    "pred=clf.predict(train_x)\n",
    "trainAccuracy = accuracy(train_y.tolist(), pred.tolist())\n",
    "print('TrainAccuracy:',trainAccuracy, '%')\n",
    "trainMetrics = metrics(train_y.tolist(), pred.tolist())\n",
    "print(trainMetrics)\n",
    "\n",
    "add_result('DecisionTree', testAccuracy, testMetrics, trainAccuracy, trainMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=SVC()\n",
    "clf.fit(train_x, train_y)\n",
    "preds=clf.predict(test_x)\n",
    "testAccuracy = accuracy(test_y.tolist(), preds.tolist())\n",
    "print('TestAccuracy:',testAccuracy, '%')\n",
    "testMetrics = metrics(test_y.tolist(), preds.tolist())\n",
    "print(testMetrics)\n",
    "\n",
    "pred=clf.predict(train_x)\n",
    "trainAccuracy = accuracy(train_y.tolist(), pred.tolist())\n",
    "print('TrainAccuracy:',trainAccuracy, '%')\n",
    "trainMetrics = metrics(train_y.tolist(), pred.tolist())\n",
    "print(trainMetrics)\n",
    "\n",
    "add_result('SVC', testAccuracy, testMetrics, trainAccuracy, trainMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_info=np.array(result_info)\n",
    "result_df=pd.DataFrame(result_info, columns=result_info_headers)\n",
    "result_df.to_csv('result_current_head.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
