{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5srCp0Z-QtUt"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from math import sqrt\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gj6S4s5HQtU3"
   },
   "outputs": [],
   "source": [
    "control_data_path=os.path.join('data', 'control')\n",
    "parkinson_data_path=os.path.join('data', 'parkinson') #Path of parkinson data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oUlc8lqwQtU4"
   },
   "outputs": [],
   "source": [
    "control_file_list=[os.path.join(control_data_path, x) for x in os.listdir(control_data_path)]\n",
    "parkinson_file_list=[os.path.join(parkinson_data_path, x) for x in os.listdir(parkinson_data_path)] #Path of parkinson data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-5jfxs3mQtU6"
   },
   "outputs": [],
   "source": [
    "header_row=[\"X\", \"Y\", \"Z\", \"Pressure\" , \"GripAngle\" , \"Timestamp\" , \"Test_ID\"]\n",
    "features_headers=['NO_STROKES_ST', 'NO_STROKES_DY', 'SPEED_ST', 'SPEED_DY', \n",
    "                  'VEL_MEAN_ST', 'VEL_MEDIAN_ST', 'VEL_STD_ST', 'VEL_PERCENTILE_1_ST', 'VEL_PERCENTILE_99_ST', \n",
    "                  'HOR_VEL_MEAN_ST', 'HOR_VEL_MEDIAN_ST', 'HOR_VEL_STD_ST', 'HOR_VEL_PERCENTILE_1_ST', 'HOR_VEL_PERCENTILE_99_ST', \n",
    "                  'VERT_VEL_MEAN_ST', 'VERT_VEL_MEDIAN_ST', 'VERT_VEL_STD_ST', 'VERT_VEL_PERCENTILE_1_ST', 'VERT_VEL_PERCENTILE_99_ST', \n",
    "                  'ACCL_MEAN_ST', 'ACCL_MEDIAN_ST', 'ACCL_STD_ST', 'ACCL_PERCENTILE_1_ST', 'ACCL_PERCENTILE_99_ST',\n",
    "                  'HOR_ACCL_MEAN_ST', 'HOR_ACCL_MEDIAN_ST', 'HOR_ACCL_STD_ST', 'HOR_ACCL_PERCENTILE_1_ST', 'HOR_ACCL_PERCENTILE_99_ST', \n",
    "                  'VERT_ACCL_MEAN_ST', 'VERT_ACCL_MEDIAN_ST', 'VERT_ACCL_STD_ST', 'VERT_ACCL_PERCENTILE_1_ST', 'VERT_ACCL_PERCENTILE_99_ST', \n",
    "                  'JERK_MEAN_ST', 'JERK_MEDIAN_ST', 'JERK_STD_ST', 'JERK_PERCENTILE_1_ST', 'JERK_PERCENTILE_99_ST', \n",
    "                  'HOR_JERK_MEAN_ST', 'HOR_JERK_MEDIAN_ST', 'HOR_JERK_STD_ST', 'HOR_JERK_PERCENTILE_1_ST', 'HOR_JERK_PERCENTILE_99_ST', \n",
    "                  'VERT_JERK_MEAN_ST', 'VERT_JERK_MEDIAN_ST', 'VERT_JERK_STD_ST', 'VERT_JERK_PERCENTILE_1_ST', 'VERT_JERK_PERCENTILE_99_ST', \n",
    "                  'VEL_MEAN_DY', 'VEL_MEDIAN_DY', 'VEL_STD_DY', 'VEL_PERCENTILE_1_DY', 'VEL_PERCENTILE_99_DY', \n",
    "                  'HOR_VEL_MEAN_DY', 'HOR_VEL_MEDIAN_DY', 'HOR_VEL_STD_DY', 'HOR_VEL_PERCENTILE_1_DY', 'HOR_VEL_PERCENTILE_99_DY', \n",
    "                  'VERT_VEL_MEAN_DY', 'VERT_VEL_MEDIAN_DY', 'VERT_VEL_STD_DY', 'VERT_VEL_PERCENTILE_1_DY', 'VERT_VEL_PERCENTILE_99_DY', \n",
    "                  'ACCL_MEAN_DY', 'ACCL_MEDIAN_DY', 'ACCL_STD_DY', 'ACCL_PERCENTILE_1_DY', 'ACCL_PERCENTILE_99_DY', \n",
    "                  'HOR_ACCL_MEAN_DY', 'HOR_ACCL_MEDIAN_DY', 'HOR_ACCL_STD_DY', 'HOR_ACCL_PERCENTILE_1_DY', 'HOR_ACCL_PERCENTILE_99_DY', \n",
    "                  'VERT_ACCL_MEAN_DY', 'VERT_ACCL_MEDIAN_DY', 'VERT_ACCL_STD_DY', 'VERT_ACCL_PERCENTILE_1_DY', 'VERT_ACCL_PERCENTILE_99_DY', \n",
    "                  'JERK_MEAN_DY', 'JERK_MEDIAN_DY', 'JERK_STD_DY', 'JERK_PERCENTILE_1_DY', 'JERK_PERCENTILE_99_DY', \n",
    "                  'HOR_JERK_MEAN_DY', 'HOR_JERK_MEDIAN_DY', 'HOR_JERK_STD_DY', 'HOR_JERK_PERCENTILE_1_DY', 'HOR_JERK_PERCENTILE_99_DY', \n",
    "                  'VERT_JERK_MEAN_DY', 'VERT_JERK_MEDIAN_DY', 'VERT_JERK_STD_DY', 'VERT_JERK_PERCENTILE_1_DY', 'VERT_JERK_PERCENTILE_99_DY',\n",
    "                  'NCV_MEAN_ST', 'NCV_MEDIAN_ST', 'NCV_STD_ST', 'NCV_PERCENTILE_1_ST', 'NCV_PERCENTILE_99_ST', \n",
    "                  'NCV_MEAN_DY', 'NCV_MEDIAN_DY', 'NCV_STD_DY', 'NCV_PERCENTILE_1_DY', 'NCV_PERCENTILE_99_DY', \n",
    "                  'NCA_MEAN_ST', 'NCA_MEDIAN_ST', 'NCA_STD_ST', 'NCA_PERCENTILE_1_ST', 'NCA_PERCENTILE_99_ST', \n",
    "                  'NCA_MEAN_DY', 'NCA_MEDIAN_DY', 'NCA_STD_DY', 'NCA_PERCENTILE_1_DY', 'NCA_PERCENTILE_99_DY',\n",
    "                  'IN_AIR_STCP', 'ON_SURFACE_ST',  'ON_SURFACE_DY',\n",
    "                  'TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FlyiHsqmQtU7"
   },
   "outputs": [],
   "source": [
    "def get_stat_data(values):\n",
    "    data = []\n",
    "    data.append(np.mean(values))\n",
    "    data.append(np.median(values))\n",
    "    data.append(np.std(values))\n",
    "    data.append(np.percentile(values, 1))\n",
    "    data.append(np.percentile(values, 99))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "atPeWlD0QtU8"
   },
   "outputs": [],
   "source": [
    "class KineticsData:\n",
    "    def __init__(self):\n",
    "        self.val = []\n",
    "        self.hor_val = []\n",
    "        self.ver_val = []\n",
    "        self.magnitude = []\n",
    "        self.hor_magnitude = []\n",
    "        self.ver_magnitude = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQlyLqGKQtU9"
   },
   "outputs": [],
   "source": [
    "def get_no_strokes(df):\n",
    "    pressure_data=df['Pressure'].values\n",
    "    on_surface = (pressure_data>600).astype(int)\n",
    "    return ((np.roll(on_surface, 1) - on_surface) != 0).astype(int).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UF3TqUZfQtVA"
   },
   "outputs": [],
   "source": [
    "def get_speed(df):\n",
    "    total_dist=0\n",
    "    duration=df['Timestamp'].values[-1]\n",
    "    coords=df[['X', 'Y', 'Z']].values\n",
    "    for i in range(10, df.shape[0]):\n",
    "        temp=np.linalg.norm(coords[i, :]-coords[i-10, :])\n",
    "        total_dist+=temp\n",
    "    speed=total_dist/duration\n",
    "    return speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tmlx0gn2QtVF"
   },
   "outputs": [],
   "source": [
    "def get_in_air_time(data):\n",
    "    data=data['Pressure'].values\n",
    "    return (data<600).astype(int).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jEBsfD_iQtVG"
   },
   "outputs": [],
   "source": [
    "def get_on_surface_time(data):\n",
    "    data=data['Pressure'].values\n",
    "    return (data>600).astype(int).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFkjNZP7QtVI"
   },
   "outputs": [],
   "source": [
    "def find_kinametics_value(data_pat):\n",
    "    timestamp_diff = []\n",
    "    \n",
    "    data_len = len(data_pat) - 10\n",
    "    vel = KineticsData()\n",
    "    \n",
    "    for t in range(0, data_len, 10):\n",
    "        timestamp_diff.append(data_pat['Timestamp'].values[t+10]-data_pat['Timestamp'].values[t])\n",
    "        \n",
    "        vel.val.append(((data_pat['X'].values[t+10] - data_pat['X'].values[t])/ timestamp_diff[-1], (data_pat['Y'].values[t+10]-data_pat['Y'].values[t])/timestamp_diff[-1]))\n",
    "        vel.hor_val.append((data_pat['X'].values[t+10] - data_pat['X'].values[t])/timestamp_diff[-1])\n",
    "        vel.ver_val.append((data_pat['Y'].values[t+10] - data_pat['Y'].values[t])/timestamp_diff[-1])\n",
    "        vel.magnitude.append(sqrt(((data_pat['X'].values[t+10]-data_pat['X'].values[t])/timestamp_diff[-1])**2 + (((data_pat['Y'].values[t+10]-data_pat['Y'].values[t])/timestamp_diff[-1])**2)))\n",
    "        \n",
    "        vel.hor_magnitude.append(abs(vel.hor_val[-1]))\n",
    "        vel.ver_magnitude.append(abs(vel.ver_val[-1]))\n",
    "    \n",
    "    data_len = len(vel.val) - 1\n",
    "    accl = KineticsData()\n",
    "    \n",
    "    for i in range(data_len):\n",
    "        accl.val.append(((vel.val[i+1][0]-vel.val[i][0])/timestamp_diff[i] , (vel.val[i+1][1]-vel.val[i][1])/timestamp_diff[i]))\n",
    "        accl.hor_val.append((vel.hor_val[i+1]-vel.hor_val[i])/timestamp_diff[i])\n",
    "        accl.ver_val.append((vel.ver_val[i+1]-vel.ver_val[i])/timestamp_diff[i])\n",
    "        accl.hor_magnitude.append(abs(accl.hor_val[-1]))\n",
    "        accl.ver_magnitude.append(abs(accl.ver_val[-1]))\n",
    "        accl.magnitude.append(sqrt(((vel.val[i+1][0]-vel.val[i][0])/timestamp_diff[i])**2 + ((vel.val[i+1][1]-vel.val[i][1])/timestamp_diff[i])**2))\n",
    "    \n",
    "    \n",
    "    data_len = len(accl.val) - 1\n",
    "    jerk = KineticsData()\n",
    "    \n",
    "    for i in range(data_len):\n",
    "        jerk.val.append(((accl.val[i+1][0]-accl.val[i][0])/timestamp_diff[i] , (accl.val[i+1][1]-accl.val[i][1])/timestamp_diff[i]))\n",
    "        jerk.hor_val.append((accl.hor_val[i+1]-accl.hor_val[i])/timestamp_diff[i])\n",
    "        jerk.ver_val.append((accl.ver_val[i+1]-accl.ver_val[i])/timestamp_diff[i])\n",
    "        jerk.hor_magnitude.append(abs(jerk.hor_val[-1]))\n",
    "        jerk.ver_magnitude.append(abs(jerk.ver_val[-1]))\n",
    "        jerk.magnitude.append(sqrt(((accl.val[i+1][0]-accl.val[i][0])/timestamp_diff[i])**2 + ((accl.val[i+1][1]-accl.val[i][1])/timestamp_diff[i])**2))\n",
    "    data = []\n",
    "    data.extend(get_stat_data(vel.magnitude))\n",
    "    data.extend(get_stat_data(vel.hor_magnitude))\n",
    "    data.extend(get_stat_data(vel.ver_magnitude))\n",
    "    data.extend(get_stat_data(accl.magnitude))\n",
    "    data.extend(get_stat_data(accl.hor_magnitude))\n",
    "    data.extend(get_stat_data(accl.ver_magnitude))\n",
    "    data.extend(get_stat_data(jerk.magnitude))\n",
    "    data.extend(get_stat_data(jerk.hor_magnitude))\n",
    "    data.extend(get_stat_data(jerk.ver_magnitude))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3HhvYxlQtVK"
   },
   "outputs": [],
   "source": [
    "def NCV_per_halfcircle(f):\n",
    "    data_pat=f\n",
    "    Vel = []\n",
    "    ncv = []\n",
    "    temp_ncv = 0\n",
    "    basex = data_pat['X'].values[0]\n",
    "    for i in range(len(data_pat)-2):\n",
    "        if data_pat['X'].values[i] == basex:\n",
    "            ncv.append(temp_ncv)\n",
    "            temp_ncv = 0\n",
    "            continue\n",
    "            \n",
    "        Vel.append(((data_pat['X'].values[i+1] - data_pat['X'].values[i])/(data_pat['Timestamp'].values[i+1]-data_pat['Timestamp'].values[i]) , (data_pat['Y'].values[i+1]-data_pat['Y'].values[i])/(data_pat['Timestamp'].values[i+1]-data_pat['Timestamp'].values[i])))\n",
    "        if Vel[-1] != (0,0):\n",
    "            temp_ncv+=1\n",
    "    ncv.append(temp_ncv)\n",
    "    #ncv = list(filter((2).__ne__, ncv))\n",
    "    ncv = ncv[ncv != 0] \n",
    "    data = get_stat_data(ncv) \n",
    "    return data\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SytkDqeuQtVK"
   },
   "outputs": [],
   "source": [
    "def NCA_per_halfcircle(data_pat):\n",
    "    timestamp_diff = []\n",
    "    Vel = []\n",
    "    data_len = len(data_pat) - 10\n",
    "    for t in range(0, data_len, 10):\n",
    "        timestamp_diff.append(data_pat['Timestamp'].values[t+10]-data_pat['Timestamp'].values[t])\n",
    "        Vel.append(((data_pat['X'].values[t+10] - data_pat['X'].values[t])/ timestamp_diff[-1], (data_pat['Y'].values[t+10]-data_pat['Y'].values[t])/timestamp_diff[-1]))\n",
    "        \n",
    "    accl = []\n",
    "    nca = []\n",
    "    temp_nca = 0\n",
    "    basex = data_pat['X'].values[0]\n",
    "    for i in range(len(Vel)-2):\n",
    "        if data_pat['X'].values[i] == basex:\n",
    "            nca.append(temp_nca)\n",
    "            #print ('tempNCa::',temp_nca)\n",
    "            temp_nca = 0\n",
    "            continue\n",
    "            \n",
    "        accl.append(((Vel[i+1][0]-Vel[i][0])/timestamp_diff[i] , (Vel[i+1][1]-Vel[i][1])/timestamp_diff[i]))\n",
    "        if accl[-1] != (0,0):\n",
    "            temp_nca+=1\n",
    "    nca.append(temp_nca)\n",
    "    nca = list(filter((2).__ne__, nca))\n",
    "    nca = nca[nca != 0] \n",
    "    data = get_stat_data(nca) \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-NwwUovLQtVM"
   },
   "outputs": [],
   "source": [
    "def generate_features(f, parkinson_target):\n",
    "    global header_row\n",
    "    df=pd.read_csv(f, sep=';', header=None, names=header_row)\n",
    "    \n",
    "    df_static=df[df[\"Test_ID\"]==0]    # static test\n",
    "    df_dynamic=df[df[\"Test_ID\"]==1]    # dynamic test\n",
    "    df_stcp=df[df[\"Test_ID\"]==2]    # STCP(Stability test on certain point)\n",
    "    #df_static_dynamic=pd.concat([df_static, df_dynamic])\n",
    "    \n",
    "    initial_timestamp=df['Timestamp'][0]\n",
    "    df['Timestamp']=df['Timestamp']- initial_timestamp # offset timestamps\n",
    "    \n",
    "    duration_static = df_static['Timestamp'].values[-1] if df_static.shape[0] else 1\n",
    "    duration_dynamic = df_dynamic['Timestamp'].values[-1] if df_dynamic.shape[0] else 1\n",
    "    duration_STCP = df_stcp['Timestamp'].values[-1] if df_stcp.shape[0] else 1\n",
    "\n",
    "    \n",
    "    data_point=[]\n",
    "    data_point.append(get_no_strokes(df_static) if df_static.shape[0] else 0) # no. of strokes for static test\n",
    "    data_point.append(get_no_strokes(df_dynamic) if df_dynamic.shape[0] else 0) # no. of strokes for dynamic test\n",
    "    data_point.append(get_speed(df_static) if df_static.shape[0] else 0) # speed for static test\n",
    "    data_point.append(get_speed(df_dynamic) if df_dynamic.shape[0] else 0) # speed for dynamic test\n",
    "\n",
    "    values = find_kinametics_value(df_static) if df_static.shape[0] else (0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)\n",
    "    data_point.extend(values)\n",
    "    values = find_kinametics_value(df_dynamic) if df_dynamic.shape[0] else (0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)\n",
    "    data_point.extend(values)\n",
    "\n",
    "    ncv=NCV_per_halfcircle(df_static) if df_static.shape[0] else (0,0,0,0,0) # NCV for static test \n",
    "    data_point.extend(ncv) \n",
    "    ncv=NCV_per_halfcircle(df_dynamic) if df_dynamic.shape[0] else (0,0,0,0,0) # NCV for dynamic test \n",
    "    data_point.extend(ncv) \n",
    "        \n",
    "    nca=NCA_per_halfcircle(df_static) if df_static.shape[0] else (0,0,0,0,0) # NCA for static test \n",
    "    data_point.extend(nca) \n",
    "    nca=NCA_per_halfcircle(df_dynamic) if df_dynamic.shape[0] else (0,0,0,0,0) # NCA for dynamic test \n",
    "    data_point.extend(nca)\n",
    "    \n",
    "    data_point.append(get_in_air_time(df_stcp) if df_stcp.shape[0] else 0) # in air time for STCP\n",
    "    data_point.append(get_on_surface_time(df_static) if df_static.shape[0] else 0) # on surface time for static test\n",
    "    data_point.append(get_on_surface_time(df_dynamic) if df_dynamic.shape[0] else 0) # on surface time for dynamic test\n",
    "    \n",
    "    data_point.append(parkinson_target)    # traget. 1 for parkinson. 0 for control.\n",
    "    \n",
    "    return data_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xRBmSvYlQtVM",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_features():\n",
    "    raw=[]\n",
    "    for x in parkinson_file_list:\n",
    "        raw.append(generate_features(x, 1))\n",
    "    for x in control_file_list:\n",
    "        raw.append(generate_features(x, 0))\n",
    "    #print(raw)\n",
    "    raw=np.array(raw)\n",
    "    #print(len(raw))\n",
    "    #print(len(raw[0]))\n",
    "    data=pd.DataFrame(raw, columns=features_headers)\n",
    "    data.fillna(0, inplace=True)\n",
    "    y=data['TARGET']\n",
    "    x=data.drop(['TARGET'], axis=1)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TQIbRe5kQtVT"
   },
   "outputs": [],
   "source": [
    "from statistics import mean, stdev\n",
    "class ClassifierScore:\n",
    "    def __init__(self, values):\n",
    "        self.values = values\n",
    "        self.max = max(values)*100\n",
    "        self.min = min(values)*100\n",
    "        self.mean = mean(values)*100\n",
    "        self.stdev = stdev(values)\n",
    "    def print(self):\n",
    "        print('List of possible accuracy:', self.values)\n",
    "        print('Maximum Accuracy:', self.max, '%')\n",
    "        print('Minimum Accuracy:', self.min , '%')\n",
    "        print('Overall Accuracy:', self.mean, '%')\n",
    "        print('Standard Deviation is:', self.stdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "def preprocess_with_minmaxscaler(x):\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    x = scaler.fit_transform(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Oversampling as control data is less than parkinson data\n",
    "#Class - SMOTE, BorderlineSMOTE, SVMSMOTE, ADASYN\n",
    "#ref - https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/\n",
    "\n",
    "from imblearn.over_sampling import ADASYN\n",
    "def oversampling_with_smote_adasyn(x,y):\n",
    "    oversample = ADASYN(random_state=3)\n",
    "    x, y = oversample.fit_resample(x, y)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DL6bfTseQtVV",
    "outputId": "79b9a46d-e757-4eb2-a733-17f9bde423da"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def select_using_selectkbest(x,y,feature_count):\n",
    "    fs = SelectKBest(score_func=mutual_info_classif, k=feature_count) # k is number of features to be selected\n",
    "    x = fs.fit_transform(x, y)\n",
    "    cols = fs.get_support(indices=True)\n",
    "    print(cols)\n",
    "    return x, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "def select_using_selectfrommodel(x, y):\n",
    "    clf = ExtraTreesClassifier(n_estimators=50)\n",
    "    clf = clf.fit(x, y)\n",
    "\n",
    "    model = SelectFromModel(clf, prefit=True)\n",
    "    X_new = model.transform(x)\n",
    "    print(X_new)\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_score(x,y,feature_count):\n",
    "    scores = fs.fit(x, y).scores_\n",
    "    barY = []\n",
    "    for i in range(feature_count):\n",
    "        barY.append(scores[cols[i]])\n",
    "    print(barY)\n",
    "    barX = []\n",
    "    for i in range(feature_count):\n",
    "        barX.append(str(cols[i]))\n",
    "    plt.bar(barX, barY)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_x, y = get_features()\n",
    "scaled_x = preprocess_with_minmaxscaler(raw_x)\n",
    "oversampled_x, y = oversampling_with_smote_adasyn(scaled_x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, selected_features_idx = select_using_selectkbest(oversampled_x,y,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qkKodRqfQtVW"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "def run_test(classifier):\n",
    "    fold = 10\n",
    "    print(fold)\n",
    "    skf = StratifiedKFold(n_splits=fold, shuffle=True, random_state=1)\n",
    "    # n_splits = number of groups for cross validation\n",
    "    predicted_targets = np.array([])\n",
    "    actual_targets = np.array([])\n",
    "    accuracy_list = list()\n",
    "    \n",
    "    for train_idx, test_idx in skf.split(x,y):\n",
    "        train_x, train_y, test_x, test_y = x[train_idx], y[train_idx], x[test_idx], y[test_idx]\n",
    "        \n",
    "        classifier.fit(train_x, train_y)\n",
    "        #scores.append(classifier.score(x_test_fold, y_test_fold))\n",
    "        \n",
    "        predicted_labels = classifier.predict(test_x)\n",
    "        accuracy = accuracy_score(test_y, predicted_labels)\n",
    "        \n",
    "        predicted_targets = np.append(predicted_targets, predicted_labels)\n",
    "        actual_targets = np.append(actual_targets, test_y)\n",
    "        accuracy_list.append(accuracy)\n",
    "        \n",
    "    ClassifierScore(accuracy_list).print()\n",
    "    plot_confusion_matrix(predicted_targets, actual_targets)\n",
    "    calculate_scores(actual_targets, predicted_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(predicted_labels_list, y_test_list):\n",
    "    cnf_matrix = confusion_matrix(y_test_list, predicted_labels_list)\n",
    "    np.set_printoptions(precision=2)\n",
    "    res_class = ['control', 'parkinson']\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    generate_confusion_matrix(cnf_matrix, classes=res_class, title='Confusion matrix, without normalization')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot normalized confusion matrix\n",
    "    #plt.figure()\n",
    "    #generate_confusion_matrix(cnf_matrix, classes=res_class, normalize=True, title='Normalized confusion matrix')\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def generate_confusion_matrix(cnf_matrix, classes, normalize=False, title='Confusion matrix'):\n",
    "    if normalize:\n",
    "        cnf_matrix = cnf_matrix.astype('float') / cnf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cnf_matrix, interpolation='nearest', cmap=plt.get_cmap('Blues'))\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cnf_matrix.max() / 2.\n",
    "\n",
    "    for i, j in itertools.product(range(cnf_matrix.shape[0]), range(cnf_matrix.shape[1])):\n",
    "        plt.text(j, i, format(cnf_matrix[i, j], fmt), horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cnf_matrix[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    return cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(y_test_list, predicted_labels_list):\n",
    "    TN, FP, FN, TP = confusion_matrix(y_test_list, predicted_labels_list).ravel()\n",
    "    sensitivity = TP / float(FN + TP) #recall\n",
    "    print(sensitivity)\n",
    "    specificity = TN / float(TN + FP)\n",
    "    print(specificity)\n",
    "    FNR =  FN/ float(FN+TP)\n",
    "    print(FNR)\n",
    "    precision = TP / float(TP + FP)\n",
    "    print(precision)\n",
    "    F1 = 2 * (precision * sensitivity) / (precision + sensitivity)\n",
    "    print(F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xZ4aaJ7bQtVW",
    "outputId": "fce6b554-bca1-4ce5-f7e7-09988e8d7fb1"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "run_test(RandomForestClassifier(n_estimators=100,criterion=\"entropy\",random_state=np.random.seed(15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TVPeJY_OQtVX",
    "outputId": "a3c36c3d-5fd7-4bdf-e7e0-a34d66f6409b"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "run_test(KNeighborsClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QLTIHr2PQtVX",
    "outputId": "10ee3de0-ad63-472c-a291-b20ce03bed93"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "run_test(DecisionTreeClassifier(criterion=\"entropy\",random_state=np.random.seed(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rlGF7sWmQtVY",
    "outputId": "c08aae02-9e22-42f7-9def-2589ca7ed590"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "run_test(AdaBoostClassifier(n_estimators=100, learning_rate=1.0, random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "run_test(ExtraTreesClassifier(n_estimators=100,criterion=\"entropy\",random_state=np.random.seed(15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "run_test(GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_ploting(f):\n",
    "    df=pd.read_csv(f, sep=';', header=None, names=header_row)\n",
    "    df_static=df[df[\"Test_ID\"]==0]\n",
    "    plt.plot(df_static['X'], df_static['Y'])\n",
    "    df_dynamic=df[df[\"Test_ID\"]==1]\n",
    "    plt.plot(df_dynamic['X'], df_dynamic['Y'])\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    #plt.savefig(\"parkinson_data\", facecolor='w', bbox_inches=\"tight\", pad_inches=0.3, transparent=True)\n",
    "    \n",
    "def get_kinametics_value(data_pat):\n",
    "    timestamp_diff = []\n",
    "    \n",
    "    data_len = len(data_pat) - 10\n",
    "    vel = KineticsData()\n",
    "    \n",
    "    for t in range(0, data_len, 10):\n",
    "        timestamp_diff.append(data_pat['Timestamp'].values[t+10]-data_pat['Timestamp'].values[t])\n",
    "        \n",
    "        vel.val.append(((data_pat['X'].values[t+10] - data_pat['X'].values[t])/ timestamp_diff[-1], (data_pat['Y'].values[t+10]-data_pat['Y'].values[t])/timestamp_diff[-1]))\n",
    "        vel.hor_val.append((data_pat['X'].values[t+10] - data_pat['X'].values[t])/timestamp_diff[-1])\n",
    "        vel.ver_val.append((data_pat['Y'].values[t+10] - data_pat['Y'].values[t])/timestamp_diff[-1])\n",
    "        vel.magnitude.append(sqrt(((data_pat['X'].values[t+10]-data_pat['X'].values[t])/timestamp_diff[-1])**2 + (((data_pat['Y'].values[t+10]-data_pat['Y'].values[t])/timestamp_diff[-1])**2)))\n",
    "        \n",
    "        vel.hor_magnitude.append(abs(vel.hor_val[-1]))\n",
    "        vel.ver_magnitude.append(abs(vel.ver_val[-1]))\n",
    "    \n",
    "    data_len = len(vel.val) - 1\n",
    "    accl = KineticsData()\n",
    "    \n",
    "    for i in range(data_len):\n",
    "        accl.val.append(((vel.val[i+1][0]-vel.val[i][0])/timestamp_diff[i] , (vel.val[i+1][1]-vel.val[i][1])/timestamp_diff[i]))\n",
    "        accl.hor_val.append((vel.hor_val[i+1]-vel.hor_val[i])/timestamp_diff[i])\n",
    "        accl.ver_val.append((vel.ver_val[i+1]-vel.ver_val[i])/timestamp_diff[i])\n",
    "        accl.hor_magnitude.append(abs(accl.hor_val[-1]))\n",
    "        accl.ver_magnitude.append(abs(accl.ver_val[-1]))\n",
    "        accl.magnitude.append(sqrt(((vel.val[i+1][0]-vel.val[i][0])/timestamp_diff[i])**2 + ((vel.val[i+1][1]-vel.val[i][1])/timestamp_diff[i])**2))\n",
    "    \n",
    "    \n",
    "    data_len = len(accl.val) - 1\n",
    "    jerk = KineticsData()\n",
    "    \n",
    "    for i in range(data_len):\n",
    "        jerk.val.append(((accl.val[i+1][0]-accl.val[i][0])/timestamp_diff[i] , (accl.val[i+1][1]-accl.val[i][1])/timestamp_diff[i]))\n",
    "        jerk.hor_val.append((accl.hor_val[i+1]-accl.hor_val[i])/timestamp_diff[i])\n",
    "        jerk.ver_val.append((accl.ver_val[i+1]-accl.ver_val[i])/timestamp_diff[i])\n",
    "        jerk.hor_magnitude.append(abs(jerk.hor_val[-1]))\n",
    "        jerk.ver_magnitude.append(abs(jerk.ver_val[-1]))\n",
    "        jerk.magnitude.append(sqrt(((accl.val[i+1][0]-accl.val[i][0])/timestamp_diff[i])**2 + ((accl.val[i+1][1]-accl.val[i][1])/timestamp_diff[i])**2))\n",
    "    return vel, accl, jerk\n",
    "\n",
    "def feature_ploting(f, test_id):\n",
    "    df=pd.read_csv(parkinson_file_list[1], sep=';', header=None, names=header_row)\n",
    "    df_static=df[df[\"Test_ID\"]==0]\n",
    "    velst, acclst, jerkst = get_kinametics_value(df_static)\n",
    "    plt.plot(jerkst.hor_val)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Jerk\")\n",
    "    #plt.savefig(\"parkinson_jerk_data_st\", facecolor='w', bbox_inches=\"tight\", pad_inches=0.3, transparent=True)\n",
    "\n",
    "def best_feature_ploting():\n",
    "    selector = SelectKBest(mutual_info_classif, k='all').fit(oversampled_x,y)\n",
    "    x_new = selector.transform(oversampled_x)\n",
    "    scores = selector.scores_\n",
    "    subplot(1,2,1)\n",
    "    plt.bar(range(0, len(scores)), scores)\n",
    "    \n",
    "    sc = []\n",
    "    sc.append(scores[70])\n",
    "    sc.append(scores[75])\n",
    "    sc.append(scores[78])\n",
    "    sc.append(scores[85])\n",
    "    sc.append(scores[90])\n",
    "\n",
    "    n = []\n",
    "    n.append(features_headers[70])\n",
    "    n.append(features_headers[75])\n",
    "    n.append(features_headers[78])\n",
    "    n.append(features_headers[85])\n",
    "    n.append(features_headers[90])\n",
    "\n",
    "    subplot(1,2,2)\n",
    "    plt.barh(n, sc)\n",
    "    #plt.savefig(\"features\", facecolor='w', bbox_inches=\"tight\", pad_inches=0.3, transparent=True)\n",
    "\n",
    "#sample_ploting(parkinson_file_list[1])\n",
    "#sample_ploting(control_file_list[1])\n",
    "\n",
    "#feature_ploting(parkinson_file_list[1], 0)\n",
    "#feature_ploting(parkinson_file_list[1], 1)\n",
    "#feature_ploting(control_file_list[1], 0)\n",
    "#feature_ploting(control_file_list[1], 1)\n",
    "#best_feature_ploting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Parkinson Detection_3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
